from __gin__ import dynamic_registration

import __main__ as train_script
from t5.data import mixtures
from t5x import models
from t5x import partitioning
from t5x import utils

import custom_tasks

include "flaxformer/t5x/configs/longt5/models/longt5_1_1_transient_global_xl.gin"
include "t5x/configs/runs/finetune.gin"

TRAIN_TOPOLOGY = 'v3-32'

MIXTURE_OR_TASK_NAME = "xsum"
TASK_FEATURE_LENGTHS = {"inputs": 1024, "targets": 128} 
TRAIN_STEPS = 1_015_000  # 1000000 pre-trained steps + 9000 fine-tuning steps.
DROPOUT_RATE = 0.1
#INITIAL_CHECKPOINT_PATH = "gs://t5-data/pretrained_models/t5x/longt5/tglobal_xl/checkpoint_1000000"
INITIAL_CHECKPOINT_PATH = "gs://jk-staging-europe-west4/t5x_jobs/t5x_job_20221026155950/checkpoint_1009000"
#LOSS_NORMALIZING_FACTOR = 116480
EVAL_PERIOD = 500
BATCH_SIZE = 128
EVALUATOR_NUM_EXAMPLES = 1000

#infer_eval/utils.DatasetConfig.task_feature_lengths = {"inputs": 1024, "targets": 128}
utils.create_learning_rate_scheduler.base_learning_rate = 0.001
partitioning.PjitPartitioner.num_partitions = 2 
partitioning.standard_logical_axis_rules:
    activation_partitioning_dims = 2
    parameter_partitioning_dims = 2 

# This is to avoid deprection warnings from Flaxformer
ACTIVATION_PARTITIONING_DIMS = None