{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running batch inference using `tfrecord` files\n",
    "\n",
    "This notebook demonstrates how to run batch inference on data in `tfrecord` format. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloads modules automatically before executing any code/script\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Import the Vertex AI SDK for Python\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import [`utils.py`](utils.py), which is a\n",
    "custom script that has utility functions to streamline configuration and\n",
    "submission of a Vertex AI custom job and to track the generated artifacts and\n",
    "metrics from the custom job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment settings\n",
    "\n",
    "Based on the [environment setup](README.md) you did previously, configure the\n",
    "following environment settings:\n",
    "\n",
    "-  `PROJECT_ID`: Configure the Google Cloud project ID.\n",
    "-  `REGION`: Configure the\n",
    "    [region](https://cloud.google.com/vertex-ai/docs/general/locations) to use\n",
    "    for Vertex AI operations throughout this notebook.\n",
    "-  `BUCKET`: Configure the Google Cloud Storage bucket name that you created\n",
    "    during environment setup. Vertex AI uses this bucket for operations such as\n",
    "    staging the code and saving generated artifacts.\n",
    "-  `TENSORBOARD_NAME`: Configure the managed TensorBoard instance name that\n",
    "    you created during the environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project definitions\n",
    "PROJECT_ID = '<YOUR PROJECT ID>' # Change to your project id.\n",
    "REGION = '<YOUR REGION>'  # Change to your region.\n",
    "\n",
    "# Bucket definitions\n",
    "BUCKET = '<YOUR BUCKET NAME>' # Change to your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard definitions\n",
    "TENSORBOARD_NAME = '<YOUR TENSORBOARD NAME>' # Change to your Tensorboard instance name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vertex AI TensorBoard ID based on name\n",
    "TENSORBOARD_ID = ! gcloud ai tensorboards list --filter=\"displayName={TENSORBOARD_NAME}\" --format=\"value(name)\" --region={REGION} 2>/dev/null \n",
    "TENSORBOARD_ID = TENSORBOARD_ID[0]\n",
    "\n",
    "print(f\"TENSORBOARD_ID = {TENSORBOARD_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the custom container image name\n",
    "IMAGE_NAME = 't5x-base' # Change to your image name\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the image exists in Container Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud container images describe $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure experiment settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = '<YOUR EXPERIMENT NAME>' # Change to your experiment name\n",
    "\n",
    "EXPERIMENT_WORKSPACE = f'gs://{BUCKET}/experiments/{EXPERIMENT_NAME}'\n",
    "EXPERIMENT_RUNS = f'{EXPERIMENT_WORKSPACE}/runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project, bucket, and\n",
    "experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=EXPERIMENT_WORKSPACE,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Configure and run a batch inference job\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a Gin file for the batch inference job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIN_FILE = '../configs/infer_t511_base_tr_en.gin'\n",
    "! cat {GIN_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Vertex AI custom job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_RUN_NAME = f'<YOUR RUN NAME>' # Change to your run name for the custom job\n",
    "INFER_RUN_ID = f'{EXPERIMENT_NAME}-{INFER_RUN_NAME}-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    "INFER_RUN_DIR = f'{EXPERIMENT_RUNS}/{INFER_RUN_ID}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the run mode as `infer` to run the T5X launch script in inference\n",
    "mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODE = 'infer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all experiment runs and run directories\n",
    "utils.get_all_experiment_run_directories(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls '<YOUR PREVIOUS RUN DIRECTORY>' # Change to the previous run directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = '<YOUR MODEL CHECKPOINT PATH>' # Change to the path where model checkpoint exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure runtime parameters for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIN_FILES = [GIN_FILE]\n",
    "GIN_OVERWRITES = [\n",
    "    'USE_CACHED_TASKS=False',\n",
    "    f'CHECKPOINT_PATH=\"{CHECKPOINT_PATH}\"',\n",
    "    f'INFER_OUTPUT_DIR=\"{INFER_RUN_DIR}\"',\n",
    "    f\"TF_EXAMPLE_FILE_PATHS=['gs://<YOUR PATH>/eval.tfrecords']\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with troubleshooting, display the values of local variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [\n",
    "    \"PROJECT_ID\", \"REGION\", \"BUCKET\", \"TENSORBOARD_NAME\", \"TENSORBOARD_ID\", \n",
    "    \"IMAGE_NAME\", \"IMAGE_URI\", \n",
    "    \"EXPERIMENT_NAME\", \"EXPERIMENT_WORKSPACE\", \"EXPERIMENT_RUNS\", \n",
    "    \"GIN_FILES\", \"GIN_OVERWRITES\", \n",
    "    \"INFER_RUN_NAME\", \"INFER_RUN_ID\", \"INFER_RUN_DIR\", \"RUN_MODE\",\n",
    "    \"CHECKPOINT_PATH\"\n",
    "]:\n",
    "    print(f\"{key}={eval(key)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Vertex AI custom job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the worker pool specification based on TPU availability in the region.\n",
    "See\n",
    "[Vertex AI locations](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)\n",
    "for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define worker pool resource specification\n",
    "# Machine to run the custom job on. For TPUs, use `cloud-tpu`.\n",
    "MACHINE_TYPE = 'cloud-tpu'\n",
    "# Accelerator type to attach to the machine. For TPUs, use `TPU_V2`, `TPU_V3`.\n",
    "ACCELERATOR_TYPE = 'TPU_V2'\n",
    "# Number of accelerators to attach to the machine.\n",
    "# For TPUs, specify the number of cores to be provisioned.\n",
    "ACCELERATOR_COUNT = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the custom job spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = utils.create_t5x_custom_job(\n",
    "    display_name=INFER_RUN_ID,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    image_uri=IMAGE_URI,\n",
    "    run_mode=RUN_MODE,\n",
    "    gin_files=GIN_FILES,\n",
    "    model_dir=CHECKPOINT_PATH,\n",
    "    gin_overwrites=GIN_OVERWRITES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.job_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the custom job to Vertex AI and track the experiment\n",
    "\n",
    "The `submit_and_track_tx5_vertex_job` function launches the T5X script in\n",
    "inference mode with the fine-tuned model checkpoint specification. The function\n",
    "submits the job to Vertex AI, which generates inferences results and writes them\n",
    "to the Cloud Storage bucket. To monitor progress of the job, click the URL in\n",
    "the cell output. The URL goes to the Vertex AI Custom Job console for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.submit_and_track_t5x_vertex_job(\n",
    "    custom_job=job,\n",
    "    job_display_name=INFER_RUN_ID,\n",
    "    run_name=INFER_RUN_ID,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    execution_name=INFER_RUN_ID,\n",
    "    model_dir=INFER_RUN_DIR,\n",
    "    vertex_ai=vertex_ai,\n",
    "    run_mode=RUN_MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore batch inference results\n",
    "\n",
    "The batch inference job writes inference results to the run directory that you\n",
    "configured. The output is written in JSON lines (`.jsonl`) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls $INFER_RUN_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is a snippet from the output of the batch inference job\n",
    "for the translation task from a fine-tuned\n",
    "[WMT English-to-German translation](https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate)\n",
    "model.\n",
    "\n",
    "``` json\n",
    "{\n",
    "\t\"inputs\": {\n",
    "\t\t\"inputs_pretokenized\": \"translate English to German: As recently as last Tuesday, the Nasdaq indices were not calculated for one hour due to data transfer errors.\",\n",
    "\t\t\"targets_pretokenized\": \"Erst am Dienstag waren die Indizes der Nasdaq wegen Fehlern im Datentransport eine Stunde lang nicht berechnet worden.\"\n",
    "\t},\n",
    "\t\"prediction\": \"Erst am vergangenen Dienstag wurden die Nasdaq-Indexe wegen Daten\\u00fcbertragungen f\\u00fcr eine Stunde nicht berechnet.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "This notebook template covers how to run a T5X job for fine-tuning, evaluation,\n",
    "and inference tasks on Vertex AI.\n",
    "\n",
    "You can copy this notebook template for a specific task or dataset or\n",
    "configuration and then make changes as needed to run the T5X job on Vertex AI.\n",
    "Refer to other notebooks in the\n",
    "[notebooks directory](/) of this repo as\n",
    "inspiration."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "802ea0518c7535cb908ec450b955e980eb8525a80af866d337675ca6fae56b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
